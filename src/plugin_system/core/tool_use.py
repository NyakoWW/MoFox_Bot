import time
from typing import List, Dict, Tuple, Optional, Any
from src.plugin_system.apis.tool_api import get_llm_available_tool_definitions, get_tool_instance
from src.plugin_system.base.base_tool import BaseTool
from src.plugin_system.core.global_announcement_manager import global_announcement_manager
from src.llm_models.utils_model import LLMRequest
from src.llm_models.payload_content import ToolCall
from src.config.config import global_config, model_config
from src.chat.utils.prompt_builder import Prompt, global_prompt_manager
from src.chat.message_receive.chat_stream import get_chat_manager
from src.common.logger import get_logger

logger = get_logger("tool_use")


def init_tool_executor_prompt():
    """åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨çš„æç¤ºè¯"""
    tool_executor_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“é—¨æ‰§è¡Œå·¥å…·çš„åŠ©æ‰‹ã€‚ä½ çš„åå­—æ˜¯{bot_name}ã€‚ç°åœ¨æ˜¯{time_now}ã€‚
ç¾¤é‡Œæ­£åœ¨è¿›è¡Œçš„èŠå¤©å†…å®¹ï¼š
{chat_history}

ç°åœ¨ï¼Œ{sender}å‘é€äº†å†…å®¹:{target_message},ä½ æƒ³è¦å›å¤taã€‚
è¯·ä»”ç»†åˆ†æèŠå¤©å†…å®¹ï¼Œè€ƒè™‘ä»¥ä¸‹å‡ ç‚¹ï¼š
1. å†…å®¹ä¸­æ˜¯å¦åŒ…å«éœ€è¦æŸ¥è¯¢ä¿¡æ¯çš„é—®é¢˜
2. æ˜¯å¦æœ‰æ˜ç¡®çš„å·¥å…·ä½¿ç”¨æŒ‡ä»¤

If you need to use a tool, please directly call the corresponding tool function. If you do not need to use any tool, simply output "No tool needed".
"""
    Prompt(tool_executor_prompt, "tool_executor_prompt")


# åˆå§‹åŒ–æç¤ºè¯
init_tool_executor_prompt()


class ToolExecutor:
    """ç‹¬ç«‹çš„å·¥å…·æ‰§è¡Œå™¨ç»„ä»¶

    å¯ä»¥ç›´æ¥è¾“å…¥èŠå¤©æ¶ˆæ¯å†…å®¹ï¼Œè‡ªåŠ¨åˆ¤æ–­å¹¶æ‰§è¡Œç›¸åº”çš„å·¥å…·ï¼Œè¿”å›ç»“æ„åŒ–çš„å·¥å…·æ‰§è¡Œç»“æœã€‚
    """

    def __init__(self, chat_id: str, enable_cache: bool = False, cache_ttl: int = 3):
        """åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨

        Args:
            executor_id: æ‰§è¡Œå™¨æ ‡è¯†ç¬¦ï¼Œç”¨äºæ—¥å¿—è®°å½•
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜æœºåˆ¶
            cache_ttl: ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆå‘¨æœŸæ•°ï¼‰
        """
        self.chat_id = chat_id
        self.chat_stream = get_chat_manager().get_stream(self.chat_id)
        self.log_prefix = f"[{get_chat_manager().get_stream_name(self.chat_id) or self.chat_id}]"

        self.llm_model = LLMRequest(model_set=model_config.model_task_config.tool_use, request_type="tool_executor")

        # ç¼“å­˜é…ç½®
        self.enable_cache = enable_cache
        self.cache_ttl = cache_ttl
        self.tool_cache = {}  # æ ¼å¼: {cache_key: {"result": result, "ttl": ttl, "timestamp": timestamp}}

        logger.info(f"{self.log_prefix}å·¥å…·æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜{'å¯ç”¨' if enable_cache else 'ç¦ç”¨'}ï¼ŒTTL={cache_ttl}")

    async def execute_from_chat_message(
        self, target_message: str, chat_history: str, sender: str, return_details: bool = False
    ) -> Tuple[List[Dict[str, Any]], List[str], str]:
        """ä»èŠå¤©æ¶ˆæ¯æ‰§è¡Œå·¥å…·

        Args:
            target_message: ç›®æ ‡æ¶ˆæ¯å†…å®¹
            chat_history: èŠå¤©å†å²
            sender: å‘é€è€…
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†ä¿¡æ¯(ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨å’Œæç¤ºè¯)

        Returns:
            å¦‚æœreturn_detailsä¸ºFalse: Tuple[List[Dict], List[str], str] - (å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨, ç©º, ç©º)
            å¦‚æœreturn_detailsä¸ºTrue: Tuple[List[Dict], List[str], str] - (ç»“æœåˆ—è¡¨, ä½¿ç”¨çš„å·¥å…·, æç¤ºè¯)
        """

        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(target_message, chat_history, sender)
        if cached_result := self._get_from_cache(cache_key):
            logger.info(f"{self.log_prefix}ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œè·³è¿‡å·¥å…·æ‰§è¡Œ")
            if not return_details:
                return cached_result, [], ""

            # ä»ç¼“å­˜ç»“æœä¸­æå–å·¥å…·åç§°
            used_tools = [result.get("tool_name", "unknown") for result in cached_result]
            return cached_result, used_tools, ""

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨
        # è·å–å¯ç”¨å·¥å…·
        tools = self._get_tool_definitions()

        # è·å–å½“å‰æ—¶é—´
        time_now = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

        bot_name = global_config.bot.nickname

        # æ„å»ºå·¥å…·è°ƒç”¨æç¤ºè¯
        prompt = await global_prompt_manager.format_prompt(
            "tool_executor_prompt",
            target_message=target_message,
            chat_history=chat_history,
            sender=sender,
            bot_name=bot_name,
            time_now=time_now,
        )

        logger.debug(f"{self.log_prefix}å¼€å§‹LLMå·¥å…·è°ƒç”¨åˆ†æ")

        # è°ƒç”¨LLMè¿›è¡Œå·¥å…·å†³ç­–
        response, (reasoning_content, model_name, tool_calls) = await self.llm_model.generate_response_async(
            prompt=prompt, tools=tools, raise_when_empty=False
        )

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_results, used_tools = await self.execute_tool_calls(tool_calls)

        # ç¼“å­˜ç»“æœ
        if tool_results:
            self._set_cache(cache_key, tool_results)

        if used_tools:
            logger.info(f"{self.log_prefix}å·¥å…·æ‰§è¡Œå®Œæˆï¼Œå…±æ‰§è¡Œ{len(used_tools)}ä¸ªå·¥å…·: {used_tools}")

        if return_details:
            return tool_results, used_tools, prompt
        else:
            return tool_results, [], ""

    def _get_tool_definitions(self) -> List[Dict[str, Any]]:
        all_tools = get_llm_available_tool_definitions()
        user_disabled_tools = global_announcement_manager.get_disabled_chat_tools(self.chat_id)
        return [definition for name, definition in all_tools if name not in user_disabled_tools]

    async def execute_tool_calls(self, tool_calls: Optional[List[ToolCall]]) -> Tuple[List[Dict[str, Any]], List[str]]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨

        Args:
            tool_calls: LLMè¿”å›çš„å·¥å…·è°ƒç”¨åˆ—è¡¨

        Returns:
            Tuple[List[Dict], List[str]]: (å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨, ä½¿ç”¨çš„å·¥å…·åç§°åˆ—è¡¨)
        """
        tool_results: List[Dict[str, Any]] = []
        used_tools = []

        if not tool_calls:
            logger.debug(f"{self.log_prefix}æ— éœ€æ‰§è¡Œå·¥å…·")
            return [], []
        
        # æå–tool_callsä¸­çš„å‡½æ•°åç§°
        func_names = [call.func_name for call in tool_calls if call.func_name]
        
        logger.info(f"{self.log_prefix}å¼€å§‹æ‰§è¡Œå·¥å…·è°ƒç”¨: {func_names}")

        # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            try:
                tool_name = tool_call.func_name
                logger.debug(f"{self.log_prefix}æ‰§è¡Œå·¥å…·: {tool_name}")

                # æ‰§è¡Œå·¥å…·
                result = await self.execute_tool_call(tool_call)

                if result:
                    tool_info = {
                        "type": result.get("type", "unknown_type"),
                        "id": result.get("id", f"tool_exec_{time.time()}"),
                        "content": result.get("content", ""),
                        "tool_name": tool_name,
                        "timestamp": time.time(),
                    }
                    content = tool_info["content"]
                    if not isinstance(content, (str, list, tuple)):
                        tool_info["content"] = str(content)

                    tool_results.append(tool_info)
                    used_tools.append(tool_name)
                    logger.info(f"{self.log_prefix}å·¥å…·{tool_name}æ‰§è¡ŒæˆåŠŸï¼Œç±»å‹: {tool_info['type']}")
                    preview = content[:200]
                    logger.debug(f"{self.log_prefix}å·¥å…·{tool_name}ç»“æœå†…å®¹: {preview}...")
            except Exception as e:
                logger.error(f"{self.log_prefix}å·¥å…·{tool_name}æ‰§è¡Œå¤±è´¥: {e}")
                # æ·»åŠ é”™è¯¯ä¿¡æ¯åˆ°ç»“æœä¸­
                error_info = {
                    "type": "tool_error",
                    "id": f"tool_error_{time.time()}",
                    "content": f"å·¥å…·{tool_name}æ‰§è¡Œå¤±è´¥: {str(e)}",
                    "tool_name": tool_name,
                    "timestamp": time.time(),
                }
                tool_results.append(error_info)

        return tool_results, used_tools

    async def execute_tool_call(self, tool_call: ToolCall, tool_instance: Optional[BaseTool] = None) -> Optional[Dict[str, Any]]:
        # sourcery skip: use-assigned-variable
        """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨

        Args:
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡

        Returns:
            Optional[Dict]: å·¥å…·è°ƒç”¨ç»“æœï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            function_name = tool_call.func_name
            function_args = tool_call.args or {}
            logger.info(f"ğŸ¤– {self.log_prefix} æ­£åœ¨æ‰§è¡Œå·¥å…·: [bold green]{function_name}[/bold green] | å‚æ•°: {function_args}")
            function_args["llm_called"] = True  # æ ‡è®°ä¸ºLLMè°ƒç”¨

            # è·å–å¯¹åº”å·¥å…·å®ä¾‹
            tool_instance = tool_instance or get_tool_instance(function_name)
            if not tool_instance:
                logger.warning(f"æœªçŸ¥å·¥å…·åç§°: {function_name}")
                return None

            # æ‰§è¡Œå·¥å…·
            result = await tool_instance.execute(function_args)
            if result:
                return {
                    "tool_call_id": tool_call.call_id,
                    "role": "tool",
                    "name": function_name,
                    "type": "function",
                    "content": result["content"],
                }
            return None
        except Exception as e:
            logger.error(f"æ‰§è¡Œå·¥å…·è°ƒç”¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise e

    def _generate_cache_key(self, target_message: str, chat_history: str, sender: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®

        Args:
            target_message: ç›®æ ‡æ¶ˆæ¯å†…å®¹
            chat_history: èŠå¤©å†å²
            sender: å‘é€è€…

        Returns:
            str: ç¼“å­˜é”®
        """
        import hashlib

        # ä½¿ç”¨æ¶ˆæ¯å†…å®¹å’Œç¾¤èŠçŠ¶æ€ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®
        content = f"{target_message}_{chat_history}_{sender}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[List[Dict]]:
        """ä»ç¼“å­˜è·å–ç»“æœ

        Args:
            cache_key: ç¼“å­˜é”®

        Returns:
            Optional[List[Dict]]: ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        if not self.enable_cache or cache_key not in self.tool_cache:
            return None

        cache_item = self.tool_cache[cache_key]
        if cache_item["ttl"] <= 0:
            # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
            del self.tool_cache[cache_key]
            logger.debug(f"{self.log_prefix}ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜é”®: {cache_key}")
            return None

        # å‡å°‘TTL
        cache_item["ttl"] -= 1
        logger.debug(f"{self.log_prefix}ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œå‰©ä½™TTL: {cache_item['ttl']}")
        return cache_item["result"]

    def _set_cache(self, cache_key: str, result: List[Dict]):
        """è®¾ç½®ç¼“å­˜

        Args:
            cache_key: ç¼“å­˜é”®
            result: è¦ç¼“å­˜çš„ç»“æœ
        """
        if not self.enable_cache:
            return

        self.tool_cache[cache_key] = {"result": result, "ttl": self.cache_ttl, "timestamp": time.time()}
        logger.debug(f"{self.log_prefix}è®¾ç½®ç¼“å­˜ï¼ŒTTL: {self.cache_ttl}")

    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
        if not self.enable_cache:
            return

        expired_keys = []
        expired_keys.extend(cache_key for cache_key, cache_item in self.tool_cache.items() if cache_item["ttl"] <= 0)
        for key in expired_keys:
            del self.tool_cache[key]

        if expired_keys:
            logger.debug(f"{self.log_prefix}æ¸…ç†äº†{len(expired_keys)}ä¸ªè¿‡æœŸç¼“å­˜")

    async def execute_specific_tool_simple(self, tool_name: str, tool_args: Dict) -> Optional[Dict]:
        """ç›´æ¥æ‰§è¡ŒæŒ‡å®šå·¥å…·

        Args:
            tool_name: å·¥å…·åç§°
            tool_args: å·¥å…·å‚æ•°
            validate_args: æ˜¯å¦éªŒè¯å‚æ•°

        Returns:
            Optional[Dict]: å·¥å…·æ‰§è¡Œç»“æœï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            tool_call = ToolCall(
                call_id=f"direct_tool_{time.time()}",
                func_name=tool_name,
                args=tool_args,
            )

            logger.info(f"{self.log_prefix}ç›´æ¥æ‰§è¡Œå·¥å…·: {tool_name}")

            result = await self.execute_tool_call(tool_call)

            if result:
                tool_info = {
                    "type": result.get("type", "unknown_type"),
                    "id": result.get("id", f"direct_tool_{time.time()}"),
                    "content": result.get("content", ""),
                    "tool_name": tool_name,
                    "timestamp": time.time(),
                }
                logger.info(f"{self.log_prefix}ç›´æ¥å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}")
                return tool_info

        except Exception as e:
            logger.error(f"{self.log_prefix}ç›´æ¥å·¥å…·æ‰§è¡Œå¤±è´¥ {tool_name}: {e}")

        return None

    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        if self.enable_cache:
            cache_count = len(self.tool_cache)
            self.tool_cache.clear()
            logger.info(f"{self.log_prefix}æ¸…ç©ºäº†{cache_count}ä¸ªç¼“å­˜é¡¹")

    def get_cache_status(self) -> Dict:
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯

        Returns:
            Dict: åŒ…å«ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if not self.enable_cache:
            return {"enabled": False, "cache_count": 0}

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired_cache()

        total_count = len(self.tool_cache)
        ttl_distribution = {}

        for cache_item in self.tool_cache.values():
            ttl = cache_item["ttl"]
            ttl_distribution[ttl] = ttl_distribution.get(ttl, 0) + 1

        return {
            "enabled": True,
            "cache_count": total_count,
            "cache_ttl": self.cache_ttl,
            "ttl_distribution": ttl_distribution,
        }

    def set_cache_config(self, enable_cache: Optional[bool] = None, cache_ttl: int = -1):
        """åŠ¨æ€ä¿®æ”¹ç¼“å­˜é…ç½®

        Args:
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_ttl: ç¼“å­˜TTL
        """
        if enable_cache is not None:
            self.enable_cache = enable_cache
            logger.info(f"{self.log_prefix}ç¼“å­˜çŠ¶æ€ä¿®æ”¹ä¸º: {'å¯ç”¨' if enable_cache else 'ç¦ç”¨'}")

        if cache_ttl > 0:
            self.cache_ttl = cache_ttl
            logger.info(f"{self.log_prefix}ç¼“å­˜TTLä¿®æ”¹ä¸º: {cache_ttl}")


"""
ToolExecutorä½¿ç”¨ç¤ºä¾‹ï¼š

# 1. åŸºç¡€ä½¿ç”¨ - ä»èŠå¤©æ¶ˆæ¯æ‰§è¡Œå·¥å…·ï¼ˆå¯ç”¨ç¼“å­˜ï¼Œé»˜è®¤TTL=3ï¼‰
executor = ToolExecutor(executor_id="my_executor")
results, _, _ = await executor.execute_from_chat_message(
    talking_message_str="ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
    is_group_chat=False
)

# 2. ç¦ç”¨ç¼“å­˜çš„æ‰§è¡Œå™¨
no_cache_executor = ToolExecutor(executor_id="no_cache", enable_cache=False)

# 3. è‡ªå®šä¹‰ç¼“å­˜TTL
long_cache_executor = ToolExecutor(executor_id="long_cache", cache_ttl=10)

# 4. è·å–è¯¦ç»†ä¿¡æ¯
results, used_tools, prompt = await executor.execute_from_chat_message(
    talking_message_str="å¸®æˆ‘æŸ¥è¯¢Pythonç›¸å…³çŸ¥è¯†",
    is_group_chat=False,
    return_details=True
)

# 5. ç›´æ¥æ‰§è¡Œç‰¹å®šå·¥å…·
result = await executor.execute_specific_tool_simple(
    tool_name="get_knowledge",
    tool_args={"query": "æœºå™¨å­¦ä¹ "}
)

# 6. ç¼“å­˜ç®¡ç†
cache_status = executor.get_cache_status()  # æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
executor.clear_cache()  # æ¸…ç©ºç¼“å­˜
executor.set_cache_config(cache_ttl=5)  # åŠ¨æ€ä¿®æ”¹ç¼“å­˜é…ç½®
"""
