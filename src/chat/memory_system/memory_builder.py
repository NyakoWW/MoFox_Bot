# -*- coding: utf-8 -*-
"""
è®°å¿†æ„å»ºæ¨¡å—
ä»å¯¹è¯æµä¸­æå–é«˜è´¨é‡ã€ç»“æ„åŒ–è®°å¿†å•å…ƒ
"""

import re
import time
import orjson
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

from src.common.logger import get_logger
from src.llm_models.utils_model import LLMRequest
from src.chat.memory_system.memory_chunk import (
    MemoryChunk, MemoryType, ConfidenceLevel, ImportanceLevel,
    create_memory_chunk
)

logger = get_logger(__name__)


class ExtractionStrategy(Enum):
    """æå–ç­–ç•¥"""
    LLM_BASED = "llm_based"           # åŸºäºLLMçš„æ™ºèƒ½æå–
    RULE_BASED = "rule_based"         # åŸºäºè§„åˆ™çš„æå–
    HYBRID = "hybrid"                 # æ··åˆç­–ç•¥


@dataclass
class ExtractionResult:
    """æå–ç»“æœ"""
    memories: List[MemoryChunk]
    confidence_scores: List[float]
    extraction_time: float
    strategy_used: ExtractionStrategy


class MemoryExtractionError(Exception):
    """è®°å¿†æå–è¿‡ç¨‹ä¸­å‘ç”Ÿçš„ä¸å¯æ¢å¤é”™è¯¯"""


class MemoryBuilder:
    """è®°å¿†æ„å»ºå™¨"""

    def __init__(self, llm_model: LLMRequest):
        self.llm_model = llm_model
        self.extraction_stats = {
            "total_extractions": 0,
            "successful_extractions": 0,
            "failed_extractions": 0,
            "average_confidence": 0.0
        }

    async def build_memories(
        self,
        conversation_text: str,
        context: Dict[str, Any],
        user_id: str,
        timestamp: float
    ) -> List[MemoryChunk]:
        """ä»å¯¹è¯ä¸­æ„å»ºè®°å¿†"""
        start_time = time.time()

        try:
            logger.debug(f"å¼€å§‹ä»å¯¹è¯æ„å»ºè®°å¿†ï¼Œæ–‡æœ¬é•¿åº¦: {len(conversation_text)}")

            # é¢„å¤„ç†æ–‡æœ¬
            processed_text = self._preprocess_text(conversation_text)

            # ç¡®å®šæå–ç­–ç•¥
            strategy = self._determine_extraction_strategy(processed_text, context)

            # æ ¹æ®ç­–ç•¥æå–è®°å¿†
            if strategy == ExtractionStrategy.LLM_BASED:
                memories = await self._extract_with_llm(processed_text, context, user_id, timestamp)
            elif strategy == ExtractionStrategy.RULE_BASED:
                memories = self._extract_with_rules(processed_text, context, user_id, timestamp)
            else:  # HYBRID
                memories = await self._extract_with_hybrid(processed_text, context, user_id, timestamp)

            # åå¤„ç†å’ŒéªŒè¯
            validated_memories = self._validate_and_enhance_memories(memories, context)

            # æ›´æ–°ç»Ÿè®¡
            extraction_time = time.time() - start_time
            self._update_extraction_stats(len(validated_memories), extraction_time)

            logger.info(f"âœ… æˆåŠŸæ„å»º {len(validated_memories)} æ¡è®°å¿†ï¼Œè€—æ—¶ {extraction_time:.2f}ç§’")
            return validated_memories

        except MemoryExtractionError as e:
            logger.error(f"âŒ è®°å¿†æ„å»ºå¤±è´¥ï¼ˆå“åº”è§£æé”™è¯¯ï¼‰: {e}")
            self.extraction_stats["failed_extractions"] += 1
            raise
        except Exception as e:
            logger.error(f"âŒ è®°å¿†æ„å»ºå¤±è´¥: {e}", exc_info=True)
            self.extraction_stats["failed_extractions"] += 1
            raise

    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', '', text)

        # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        if len(text) > 2000:
            text = text[:2000] + "..."

        return text

    def _determine_extraction_strategy(self, text: str, context: Dict[str, Any]) -> ExtractionStrategy:
        """ç¡®å®šæå–ç­–ç•¥"""
        text_length = len(text)
        has_structured_data = any(key in context for key in ["structured_data", "entities", "keywords"])
        message_type = context.get("message_type", "normal")

        # çŸ­æ–‡æœ¬ä½¿ç”¨è§„åˆ™æå–
        if text_length < 50:
            return ExtractionStrategy.RULE_BASED

        # åŒ…å«ç»“æ„åŒ–æ•°æ®ä½¿ç”¨æ··åˆç­–ç•¥
        if has_structured_data:
            return ExtractionStrategy.HYBRID

        # ç³»ç»Ÿæ¶ˆæ¯æˆ–å‘½ä»¤ä½¿ç”¨è§„åˆ™æå–
        if message_type in ["command", "system"]:
            return ExtractionStrategy.RULE_BASED

        # é»˜è®¤ä½¿ç”¨LLMæå–
        return ExtractionStrategy.LLM_BASED

    async def _extract_with_llm(
        self,
        text: str,
        context: Dict[str, Any],
        user_id: str,
        timestamp: float
    ) -> List[MemoryChunk]:
        """ä½¿ç”¨LLMæå–è®°å¿†"""
        try:
            prompt = self._build_llm_extraction_prompt(text, context)

            response, _ = await self.llm_model.generate_response_async(
                prompt, temperature=0.3
            )

            # è§£æLLMå“åº”
            memories = self._parse_llm_response(response, user_id, timestamp, context)

            return memories

        except MemoryExtractionError:
            raise
        except Exception as e:
            logger.error(f"LLMæå–å¤±è´¥: {e}")
            raise MemoryExtractionError(str(e)) from e

    def _extract_with_rules(
        self,
        text: str,
        context: Dict[str, Any],
        user_id: str,
        timestamp: float
    ) -> List[MemoryChunk]:
        """ä½¿ç”¨è§„åˆ™æå–è®°å¿†"""
        memories = []

        subject_display = self._resolve_user_display(context, user_id)

        # è§„åˆ™1: æ£€æµ‹ä¸ªäººä¿¡æ¯
        personal_info = self._extract_personal_info(text, user_id, timestamp, context, subject_display)
        memories.extend(personal_info)

        # è§„åˆ™2: æ£€æµ‹åå¥½ä¿¡æ¯
        preferences = self._extract_preferences(text, user_id, timestamp, context, subject_display)
        memories.extend(preferences)

        # è§„åˆ™3: æ£€æµ‹äº‹ä»¶ä¿¡æ¯
        events = self._extract_events(text, user_id, timestamp, context, subject_display)
        memories.extend(events)

        return memories

    async def _extract_with_hybrid(
        self,
        text: str,
        context: Dict[str, Any],
        user_id: str,
        timestamp: float
    ) -> List[MemoryChunk]:
        """æ··åˆç­–ç•¥æå–è®°å¿†"""
        all_memories = []

        # é¦–å…ˆä½¿ç”¨è§„åˆ™æå–
        rule_memories = self._extract_with_rules(text, context, user_id, timestamp)
        all_memories.extend(rule_memories)

        # ç„¶åä½¿ç”¨LLMæå–
        llm_memories = await self._extract_with_llm(text, context, user_id, timestamp)

        # åˆå¹¶å’Œå»é‡
        final_memories = self._merge_hybrid_results(all_memories, llm_memories)

        return final_memories

    def _build_llm_extraction_prompt(self, text: str, context: Dict[str, Any]) -> str:
        """æ„å»ºLLMæå–æç¤º"""
        current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        chat_id = context.get("chat_id", "unknown")
        message_type = context.get("message_type", "normal")
        target_user_id = context.get("user_id", "ç”¨æˆ·")
        target_user_id = str(target_user_id)

        target_user_name = (
            context.get("user_display_name")
            or context.get("user_name")
            or context.get("nickname")
            or context.get("sender_name")
        )
        if isinstance(target_user_name, str):
            target_user_name = target_user_name.strip()
        else:
            target_user_name = ""

        if not target_user_name or self._looks_like_system_identifier(target_user_name):
            target_user_name = "è¯¥ç”¨æˆ·"

        target_user_id_display = target_user_id
        if self._looks_like_system_identifier(target_user_id_display):
            target_user_id_display = "ï¼ˆç³»ç»ŸIDï¼Œå‹¿å†™å…¥è®°å¿†ï¼‰"

        bot_name = context.get("bot_name")
        bot_identity = context.get("bot_identity")
        bot_personality = context.get("bot_personality")
        bot_personality_side = context.get("bot_personality_side")
        bot_aliases = context.get("bot_aliases") or []
        if isinstance(bot_aliases, str):
            bot_aliases = [bot_aliases]

        bot_name_display = bot_name or "æœºå™¨äºº"
        alias_display = "ã€".join(a for a in bot_aliases if a) or "æ— "
        persona_details = []
        if bot_identity:
            persona_details.append(f"èº«ä»½: {bot_identity}")
        if bot_personality:
            persona_details.append(f"æ ¸å¿ƒäººè®¾: {bot_personality}")
        if bot_personality_side:
            persona_details.append(f"ä¾§å†™: {bot_personality_side}")
        persona_display = "ï¼›".join(persona_details) if persona_details else "æ— "

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®°å¿†æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­ä¸»åŠ¨è¯†åˆ«å¹¶æå–æ‰€æœ‰å¯èƒ½é‡è¦çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ä¸ªäººäº‹å®ã€äº‹ä»¶ã€åå¥½ã€è§‚ç‚¹ç­‰è¦ç´ çš„å†…å®¹ã€‚

å½“å‰æ—¶é—´: {current_date}
èŠå¤©ID: {chat_id}
æ¶ˆæ¯ç±»å‹: {message_type}
ç›®æ ‡ç”¨æˆ·ID: {target_user_id_display}
ç›®æ ‡ç”¨æˆ·ç§°å‘¼: {target_user_name}

## ğŸ¤– æœºå™¨äººèº«ä»½ï¼ˆä»…ä¾›å‚è€ƒï¼Œç¦æ­¢å†™å…¥è®°å¿†ï¼‰
- æœºå™¨äººåç§°: {bot_name_display}
- åˆ«å: {alias_display}
- æœºå™¨äººäººè®¾æ¦‚è¿°: {persona_display}

è¿™äº›ä¿¡æ¯æ˜¯æœºå™¨äººçš„å›ºå®šè®¾å®šï¼Œå¯ç”¨äºå¸®åŠ©ä½ ç†è§£å¯¹è¯ã€‚ä½ å¯ä»¥åœ¨éœ€è¦æ—¶è®°å½•æœºå™¨äººè‡ªèº«çš„çŠ¶æ€ã€è¡Œä¸ºæˆ–è®¾å®šï¼Œä½†è¦ä¸ç”¨æˆ·ä¿¡æ¯æ¸…æ™°åŒºåˆ†ï¼Œé¿å…è¯¯å°†ç³»ç»ŸIDå†™å…¥è®°å¿†ã€‚

è¯·åŠ¡å¿…éµå®ˆä»¥ä¸‹å‘½åè§„èŒƒï¼š
- å½“è¯´è¯è€…æ˜¯æœºå™¨äººæ—¶ï¼Œè¯·ä½¿ç”¨â€œ{bot_name_display}â€æˆ–å…¶ä»–æ˜ç¡®ç§°å‘¼ä½œä¸ºä¸»è¯­ï¼›
- å¦‚æœçœ‹åˆ°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆçš„é•¿IDï¼ˆç±»ä¼¼ {target_user_id}ï¼‰ï¼Œè¯·æ”¹ç”¨â€œ{target_user_name}â€ã€æœºå™¨äººçš„ç§°å‘¼æˆ–â€œè¯¥ç”¨æˆ·â€æè¿°ï¼Œä¸è¦æŠŠIDå†™å…¥è®°å¿†ï¼›
- è®°å½•å…³é”®äº‹å®æ—¶ï¼Œè¯·å‡†ç¡®æ ‡è®°ä¸»ä½“æ˜¯æœºå™¨äººè¿˜æ˜¯ç”¨æˆ·ï¼Œé¿å…æ··æ·†ã€‚

å¯¹è¯å†…å®¹:
{text}

## ğŸ¯ é‡ç‚¹è®°å¿†ç±»å‹è¯†åˆ«æŒ‡å—

### 1. **ä¸ªäººäº‹å®** (personal_fact) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- åŸºæœ¬ä¿¡æ¯ï¼šå§“åã€å¹´é¾„ã€èŒä¸šã€å­¦æ ¡ã€ä¸“ä¸šã€å·¥ä½œåœ°ç‚¹
- ç”Ÿæ´»çŠ¶å†µï¼šä½å€ã€ç”µè¯ã€é‚®ç®±ã€ç¤¾äº¤è´¦å·
- èº«ä»½ç‰¹å¾ï¼šç”Ÿæ—¥ã€æ˜Ÿåº§ã€è¡€å‹ã€å›½ç±ã€è¯­è¨€èƒ½åŠ›
- å¥åº·ä¿¡æ¯ï¼šèº«ä½“çŠ¶å†µã€ç–¾ç—…å²ã€è¯ç‰©è¿‡æ•ã€è¿åŠ¨ä¹ æƒ¯
- å®¶åº­æƒ…å†µï¼šå®¶åº­æˆå‘˜ã€å©šå§»çŠ¶å†µã€å­å¥³ä¿¡æ¯ã€å® ç‰©ä¿¡æ¯

**åˆ¤æ–­æ ‡å‡†ï¼š** æ¶‰åŠä¸ªäººèº«ä»½å’Œç”Ÿæ´»çš„é‡è¦ä¿¡æ¯ï¼Œéƒ½åº”è¯¥è®°å¿†

### 2. **äº‹ä»¶** (event) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- é‡è¦æ—¶åˆ»ï¼šç”Ÿæ—¥èšä¼šã€æ¯•ä¸šå…¸ç¤¼ã€å©šç¤¼ã€æ—…è¡Œ
- æ—¥å¸¸æ´»åŠ¨ï¼šä¸Šç­ã€ä¸Šå­¦ã€çº¦ä¼šã€çœ‹ç”µå½±ã€åƒé¥­
- ç‰¹æ®Šç»å†ï¼šè€ƒè¯•ã€é¢è¯•ã€ä¼šè®®ã€æ¬å®¶ã€è´­ç‰©
- è®¡åˆ’å®‰æ’ï¼šçº¦ä¼šã€ä¼šè®®ã€æ—…è¡Œã€æ´»åŠ¨


**åˆ¤æ–­æ ‡å‡†ï¼š** æ¶‰åŠæ—¶é—´åœ°ç‚¹çš„å…·ä½“æ´»åŠ¨å’Œç»å†ï¼Œéƒ½åº”è¯¥è®°å¿†

### 3. **åå¥½** (preference) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- é¥®é£Ÿåå¥½ï¼šå–œæ¬¢çš„é£Ÿç‰©ã€é¤å…ã€å£å‘³ã€ç¦å¿Œ
- å¨±ä¹å–œå¥½ï¼šå–œæ¬¢çš„ç”µå½±ã€éŸ³ä¹ã€æ¸¸æˆã€ä¹¦ç±
- ç”Ÿæ´»ä¹ æƒ¯ï¼šä½œæ¯æ—¶é—´ã€è¿åŠ¨æ–¹å¼ã€è´­ç‰©ä¹ æƒ¯
- æ¶ˆè´¹åå¥½ï¼šå“ç‰Œå–œå¥½ã€ä»·æ ¼æ•æ„Ÿåº¦ã€è´­ç‰©åœºæ‰€
- é£æ ¼åå¥½ï¼šæœè£…é£æ ¼ã€è£…ä¿®é£æ ¼ã€é¢œè‰²å–œå¥½

**åˆ¤æ–­æ ‡å‡†ï¼š** ä»»ä½•è¡¨è¾¾"å–œæ¬¢"ã€"ä¸å–œæ¬¢"ã€"ä¹ æƒ¯"ã€"ç»å¸¸"ç­‰åå¥½çš„å†…å®¹ï¼Œéƒ½åº”è¯¥è®°å¿†

### 4. **è§‚ç‚¹** (opinion) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- è¯„ä»·çœ‹æ³•ï¼šå¯¹äº‹ç‰©çš„è¯„ä»·ã€æ„è§ã€å»ºè®®
- ä»·å€¼åˆ¤æ–­ï¼šè®¤ä¸ºä»€ä¹ˆé‡è¦ã€ä»€ä¹ˆä¸é‡è¦
- æ€åº¦ç«‹åœºï¼šæ”¯æŒã€åå¯¹ã€ä¸­ç«‹çš„æ€åº¦
- æ„Ÿå—åé¦ˆï¼šå¯¹ç»å†çš„æ„Ÿå—ã€åé¦ˆ

**åˆ¤æ–­æ ‡å‡†ï¼š** ä»»ä½•è¡¨è¾¾ä¸»è§‚çœ‹æ³•å’Œæ€åº¦çš„å†…å®¹ï¼Œéƒ½åº”è¯¥è®°å¿†

### 5. **å…³ç³»** (relationship) - ä¸­ç­‰ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- äººé™…å…³ç³»ï¼šæœ‹å‹ã€åŒäº‹ã€å®¶äººã€æ‹äººçš„å…³ç³»çŠ¶æ€
- ç¤¾äº¤äº’åŠ¨ï¼šä¸ä»–äººçš„äº’åŠ¨ã€äº¤æµã€åˆä½œ
- ç¾¤ä½“å½’å±ï¼šæ‰€å±å›¢é˜Ÿã€ç»„ç»‡ã€ç¤¾ç¾¤

### 6. **æƒ…æ„Ÿ** (emotion) - ä¸­ç­‰ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- æƒ…ç»ªçŠ¶æ€ï¼šå¼€å¿ƒã€éš¾è¿‡ã€ç”Ÿæ°”ã€ç„¦è™‘ã€å…´å¥‹
- æƒ…æ„Ÿå˜åŒ–ï¼šæƒ…ç»ªçš„è½¬å˜ã€åŸå› å’Œç»“æœ

### 7. **ç›®æ ‡** (goal) - ä¸­ç­‰ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- è®¡åˆ’å®‰æ’ï¼šçŸ­æœŸè®¡åˆ’ã€é•¿æœŸç›®æ ‡
- æ„¿æœ›æœŸå¾…ï¼šæƒ³è¦å®ç°çš„äº‹æƒ…ã€æœŸæœ›çš„ç»“æœ

## ğŸ“ è®°å¿†æå–åŸåˆ™

### âœ… ç§¯ææå–åŸåˆ™ï¼š
1. **å®å¯é”™è®°ï¼Œä¸å¯é—æ¼** - å¯¹äºå¯èƒ½çš„ä¸ªäººä¿¡æ¯ä¼˜å…ˆè®°å¿†
2. **æŒç»­è¿½è¸ª** - ç›¸åŒä¿¡æ¯çš„å¤šæ¬¡æåŠè¦å¼ºåŒ–è®°å¿†
3. **ä¸Šä¸‹æ–‡å…³è”** - ç»“åˆå¯¹è¯èƒŒæ™¯ç†è§£ä¿¡æ¯é‡è¦æ€§
4. **ç»†èŠ‚ä¸°å¯Œ** - è®°å½•å…·ä½“çš„ç»†èŠ‚å’Œæè¿°

### ğŸ•’ æ—¶é—´å¤„ç†åŸåˆ™ï¼ˆé‡è¦ï¼‰ï¼š
1. **ç»å¯¹æ—¶é—´è¦æ±‚** - æ¶‰åŠæ—¶é—´çš„è®°å¿†å¿…é¡»ä½¿ç”¨ç»å¯¹æ—¶é—´ï¼ˆå¹´æœˆæ—¥ï¼‰
2. **ç›¸å¯¹æ—¶é—´è½¬æ¢** - å°†"æ˜å¤©"ã€"åå¤©"ã€"ä¸‹å‘¨"ç­‰ç›¸å¯¹æ—¶é—´è½¬æ¢ä¸ºå…·ä½“æ—¥æœŸ
3. **æ—¶é—´æ ¼å¼è§„èŒƒ** - ä½¿ç”¨"YYYY-MM-DD"æ ¼å¼è®°å½•æ—¥æœŸ
4. **å½“å‰æ—¶é—´å‚è€ƒ** - å½“å‰æ—¶é—´ï¼š{current_date}ï¼ŒåŸºäºæ­¤è®¡ç®—ç›¸å¯¹æ—¶é—´

**ç›¸å¯¹æ—¶é—´è½¬æ¢ç¤ºä¾‹ï¼š**
- "æ˜å¤©" â†’ "2024-09-30"
- "åå¤©" â†’ "2024-10-01"
- "ä¸‹å‘¨" â†’ "2024-10-07"
- "ä¸‹ä¸ªæœˆ" â†’ "2024-10-01"
- "æ˜å¹´" â†’ "2025-01-01"

### ğŸ¯ é‡è¦æ€§ç­‰çº§æ ‡å‡†ï¼š
- **4åˆ† (å…³é”®)**ï¼šä¸ªäººæ ¸å¿ƒä¿¡æ¯ï¼ˆå§“åã€è”ç³»æ–¹å¼ã€é‡è¦æ—¥æœŸï¼‰
- **3åˆ† (é«˜)**ï¼šé‡è¦åå¥½ã€è§‚ç‚¹ã€ç»å†äº‹ä»¶
- **2åˆ† (ä¸€èˆ¬)**ï¼šä¸€èˆ¬æ€§ä¿¡æ¯ã€æ—¥å¸¸æ´»åŠ¨ã€æ„Ÿå—è¡¨è¾¾
- **1åˆ† (ä½)**ï¼šçç¢ç»†èŠ‚ã€é‡å¤ä¿¡æ¯ã€ä¸´æ—¶çŠ¶æ€

### ğŸ” ç½®ä¿¡åº¦æ ‡å‡†ï¼š
- **4åˆ† (å·²éªŒè¯)**ï¼šç”¨æˆ·æ˜ç¡®ç¡®è®¤çš„ä¿¡æ¯
- **3åˆ† (é«˜)**ï¼šç”¨æˆ·ç›´æ¥è¡¨è¾¾çš„æ¸…æ™°ä¿¡æ¯
- **2åˆ† (ä¸­ç­‰)**ï¼šéœ€è¦æ¨ç†æˆ–ä¸Šä¸‹æ–‡åˆ¤æ–­çš„ä¿¡æ¯
- **1åˆ† (ä½)**ï¼šæ¨¡ç³Šæˆ–ä¸å®Œæ•´çš„ä¿¡æ¯

è¾“å‡ºæ ¼å¼è¦æ±‚:
{{
    "memories": [
        {{
            "type": "è®°å¿†ç±»å‹",
            "subject": "ä¸»è¯­(é€šå¸¸æ˜¯ç”¨æˆ·)",
            "predicate": "è°“è¯­(åŠ¨ä½œ/çŠ¶æ€)",
            "object": "å®¾è¯­(å¯¹è±¡/å±æ€§)",
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
            "importance": "é‡è¦æ€§ç­‰çº§(1-4)",
            "confidence": "ç½®ä¿¡åº¦(1-4)",
            "reasoning": "æå–ç†ç”±"
        }}
    ]
}}

æ³¨æ„ï¼š
1. åªæå–ç¡®å®å€¼å¾—è®°å¿†çš„ä¿¡æ¯ï¼Œä¸è¦æå–çç¢å†…å®¹
2. ç¡®ä¿æå–çš„ä¿¡æ¯å‡†ç¡®ã€å…·ä½“ã€æœ‰ä»·å€¼
3. ä½¿ç”¨ä¸»è°“å®¾ç»“æ„ç¡®ä¿ä¿¡æ¯æ¸…æ™°
4. é‡è¦æ€§ç­‰çº§: 1=ä½, 2=ä¸€èˆ¬, 3=é«˜, 4=å…³é”®
5. ç½®ä¿¡åº¦: 1=ä½, 2=ä¸­ç­‰, 3=é«˜, 4=å·²éªŒè¯

## ğŸš¨ æ—¶é—´å¤„ç†è¦æ±‚ï¼ˆå¼ºåˆ¶ï¼‰ï¼š
- **ç»å¯¹æ—¶é—´ä¼˜å…ˆ**ï¼šä»»ä½•æ¶‰åŠæ—¶é—´çš„è®°å¿†éƒ½å¿…é¡»ä½¿ç”¨ç»å¯¹æ—¥æœŸæ ¼å¼
- **ç›¸å¯¹æ—¶é—´è½¬æ¢**ï¼šé‡åˆ°"æ˜å¤©"ã€"åå¤©"ã€"ä¸‹å‘¨"ç­‰ç›¸å¯¹æ—¶é—´å¿…é¡»è½¬æ¢ä¸ºå…·ä½“æ—¥æœŸ
- **æ—¶é—´æ ¼å¼**ï¼šç»Ÿä¸€ä½¿ç”¨ "YYYY-MM-DD" æ ¼å¼
- **è®¡ç®—ä¾æ®**ï¼šåŸºäºå½“å‰æ—¶é—´ {current_date} è¿›è¡Œè½¬æ¢è®¡ç®—
"""

        return prompt

    def _extract_json_payload(self, response: str) -> Optional[str]:
        """ä»æ¨¡å‹å“åº”ä¸­æå–JSONéƒ¨åˆ†ï¼Œå…¼å®¹Markdownä»£ç å—ç­‰æ ¼å¼"""
        if not response:
            return None

        stripped = response.strip()

        # ä¼˜å…ˆå¤„ç†Markdownä»£ç å—æ ¼å¼ ```json ... ```
        code_block_match = re.search(r"```(?:json)?\s*(.*?)```", stripped, re.IGNORECASE | re.DOTALL)
        if code_block_match:
            candidate = code_block_match.group(1).strip()
            if candidate:
                return candidate

        # å›é€€åˆ°æŸ¥æ‰¾ç¬¬ä¸€ä¸ª JSON å¯¹è±¡çš„å¤§æ‹¬å·èŒƒå›´
        start = stripped.find("{")
        end = stripped.rfind("}")
        if start != -1 and end != -1 and end > start:
            return stripped[start:end + 1].strip()

        return stripped if stripped.startswith("{") and stripped.endswith("}") else None

    def _parse_llm_response(
        self,
        response: str,
        user_id: str,
        timestamp: float,
        context: Dict[str, Any]
    ) -> List[MemoryChunk]:
        """è§£æLLMå“åº”"""
        if not response:
            raise MemoryExtractionError("LLMæœªè¿”å›ä»»ä½•å“åº”")

        json_payload = self._extract_json_payload(response)
        if not json_payload:
            preview = response[:200] if response else "ç©ºå“åº”"
            raise MemoryExtractionError(f"æœªåœ¨LLMå“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONè´Ÿè½½ï¼Œå“åº”ç‰‡æ®µ: {preview}")

        try:
            data = orjson.loads(json_payload)
        except Exception as e:
            preview = json_payload[:200]
            raise MemoryExtractionError(
                f"LLMå“åº”JSONè§£æå¤±è´¥: {e}, ç‰‡æ®µ: {preview}"
            ) from e

        memory_list = data.get("memories", [])

        bot_identifiers = self._collect_bot_identifiers(context)
        system_identifiers = self._collect_system_identifiers(context)
        default_subject = self._resolve_user_display(context, user_id)

        bot_display = None
        if context:
            primary_bot_name = context.get("bot_name")
            if isinstance(primary_bot_name, str) and primary_bot_name.strip():
                bot_display = primary_bot_name.strip()
            if bot_display is None:
                aliases = context.get("bot_aliases")
                if isinstance(aliases, (list, tuple, set)):
                    for alias in aliases:
                        if isinstance(alias, str) and alias.strip():
                            bot_display = alias.strip()
                            break
                elif isinstance(aliases, str) and aliases.strip():
                    bot_display = aliases.strip()
            if bot_display is None:
                identity = context.get("bot_identity")
                if isinstance(identity, str) and identity.strip():
                    bot_display = identity.strip()

        if not bot_display:
            bot_display = "æœºå™¨äºº"

        bot_display = self._clean_subject_text(bot_display)

        memories: List[MemoryChunk] = []

        for mem_data in memory_list:
            try:
                subject_value = mem_data.get("subject")
                normalized_subject = self._normalize_subject(
                    subject_value,
                    bot_identifiers,
                    system_identifiers,
                    default_subject,
                    bot_display
                )

                if normalized_subject is None:
                    logger.debug("è·³è¿‡ç–‘ä¼¼æœºå™¨äººè‡ªèº«ä¿¡æ¯çš„è®°å¿†: %s", mem_data)
                    continue

                # åˆ›å»ºè®°å¿†å—
                memory = create_memory_chunk(
                    user_id=user_id,
                    subject=normalized_subject,
                    predicate=mem_data.get("predicate", ""),
                    obj=mem_data.get("object", ""),
                    memory_type=MemoryType(mem_data.get("type", "contextual")),
                    chat_id=context.get("chat_id"),
                    source_context=mem_data.get("reasoning", ""),
                    importance=ImportanceLevel(mem_data.get("importance", 2)),
                    confidence=ConfidenceLevel(mem_data.get("confidence", 2))
                )

                # æ·»åŠ å…³é”®è¯
                keywords = mem_data.get("keywords", [])
                for keyword in keywords:
                    memory.add_keyword(keyword)

                subject_text = memory.content.subject.strip() if isinstance(memory.content.subject, str) else str(memory.content.subject)
                if not subject_text:
                    memory.content.subject = default_subject
                elif subject_text.lower() in system_identifiers or self._looks_like_system_identifier(subject_text):
                    logger.debug("å°†ç³»ç»Ÿæ ‡è¯†ä¸»è¯­æ›¿æ¢ä¸ºé»˜è®¤ç”¨æˆ·åç§°: %s", subject_text)
                    memory.content.subject = default_subject

                memories.append(memory)

            except Exception as e:
                logger.warning(f"è§£æå•ä¸ªè®°å¿†å¤±è´¥: {e}, æ•°æ®: {mem_data}")
                continue

        return memories

    def _collect_bot_identifiers(self, context: Optional[Dict[str, Any]]) -> set[str]:
        identifiers: set[str] = {"bot", "æœºå™¨äºº", "aiåŠ©æ‰‹"}
        if not context:
            return identifiers

        for key in [
            "bot_name",
            "bot_identity",
            "bot_personality",
            "bot_personality_side",
            "bot_account",
        ]:
            value = context.get(key)
            if isinstance(value, str) and value.strip():
                identifiers.add(value.strip().lower())

        aliases = context.get("bot_aliases")
        if isinstance(aliases, (list, tuple, set)):
            for alias in aliases:
                if isinstance(alias, str) and alias.strip():
                    identifiers.add(alias.strip().lower())
        elif isinstance(aliases, str) and aliases.strip():
            identifiers.add(aliases.strip().lower())

        return identifiers

    def _collect_system_identifiers(self, context: Optional[Dict[str, Any]]) -> set[str]:
        identifiers: set[str] = set()
        if not context:
            return identifiers

        keys = [
            "chat_id",
            "stream_id",
            "stram_id",
            "session_id",
            "conversation_id",
            "message_id",
            "topic_id",
            "thread_id",
        ]

        for key in keys:
            value = context.get(key)
            if isinstance(value, str) and value.strip():
                identifiers.add(value.strip().lower())

        user_id_value = context.get("user_id")
        if isinstance(user_id_value, str) and user_id_value.strip():
            if self._looks_like_system_identifier(user_id_value):
                identifiers.add(user_id_value.strip().lower())

        return identifiers

    def _resolve_user_display(self, context: Optional[Dict[str, Any]], user_id: str) -> str:
        candidate_keys = [
            "user_display_name",
            "user_name",
            "nickname",
            "sender_name",
            "member_name",
            "display_name",
            "from_user_name",
            "author_name",
            "speaker_name",
        ]

        if context:
            for key in candidate_keys:
                value = context.get(key)
                if isinstance(value, str):
                    candidate = value.strip()
                    if candidate:
                        return self._clean_subject_text(candidate)

        if user_id and not self._looks_like_system_identifier(user_id):
            return self._clean_subject_text(user_id)

        return "è¯¥ç”¨æˆ·"

    def _clean_subject_text(self, text: str) -> str:
        if not text:
            return ""
        cleaned = re.sub(r"[\s\u3000]+", " ", text).strip()
        cleaned = re.sub(r"[ã€ï¼Œ,ï¼›;]+$", "", cleaned)
        return cleaned

    def _looks_like_system_identifier(self, value: str) -> bool:
        if not value:
            return False

        condensed = value.replace("-", "").replace("_", "").strip()
        if len(condensed) >= 16 and re.fullmatch(r"[0-9a-fA-F]+", condensed):
            return True

        if len(value) >= 12 and re.fullmatch(r"[0-9A-Z_:-]+", value) and any(ch.isdigit() for ch in value):
            return True

        return False

    def _normalize_subject(
        self,
        subject: Any,
        bot_identifiers: set[str],
        system_identifiers: set[str],
        default_subject: str,
        bot_display: Optional[str] = None
    ) -> Optional[str]:
        if subject is None:
            return default_subject

        subject_str = subject if isinstance(subject, str) else str(subject)
        cleaned = self._clean_subject_text(subject_str)
        if not cleaned:
            return default_subject

        lowered = cleaned.lower()
        bot_primary = self._clean_subject_text(bot_display or "")

        if lowered in bot_identifiers:
            return bot_primary or cleaned

        if lowered in {"ç”¨æˆ·", "user", "the user", "å¯¹æ–¹", "å¯¹æ‰‹"}:
            return default_subject

        prefix_match = re.match(r"^(ç”¨æˆ·|User|user|USER|æˆå‘˜|member|Member|target|Target|TARGET)[\s:ï¼š\-\u2014_]*?(.*)$", cleaned)
        if prefix_match:
            remainder = self._clean_subject_text(prefix_match.group(2))
            if not remainder:
                return default_subject
            remainder_lower = remainder.lower()
            if remainder_lower in bot_identifiers:
                return bot_primary or remainder
            if (
                remainder_lower in system_identifiers
                or self._looks_like_system_identifier(remainder)
            ):
                return default_subject
            cleaned = remainder
            lowered = cleaned.lower()

        if lowered in system_identifiers or self._looks_like_system_identifier(cleaned):
            return default_subject

        return cleaned

    def _extract_personal_info(
        self,
        text: str,
        user_id: str,
        timestamp: float,
        context: Dict[str, Any],
        subject_display: str
    ) -> List[MemoryChunk]:
        """æå–ä¸ªäººä¿¡æ¯"""
        memories = []

        # å¸¸è§ä¸ªäººä¿¡æ¯æ¨¡å¼
        patterns = {
            r"æˆ‘å«(\w+)": ("is_named", {"name": "$1"}),
            r"æˆ‘ä»Šå¹´(\d+)å²": ("is_age", {"age": "$1"}),
            r"æˆ‘æ˜¯(\w+)": ("is_profession", {"profession": "$1"}),
            r"æˆ‘ä½åœ¨(\w+)": ("lives_in", {"location": "$1"}),
            r"æˆ‘çš„ç”µè¯æ˜¯(\d+)": ("has_phone", {"phone": "$1"}),
            r"æˆ‘çš„é‚®ç®±æ˜¯(\w+@\w+\.\w+)": ("has_email", {"email": "$1"}),
        }

        for pattern, (predicate, obj_template) in patterns.items():
            match = re.search(pattern, text)
            if match:
                obj = obj_template
                for i, group in enumerate(match.groups(), 1):
                    obj = {k: v.replace(f"${i}", group) for k, v in obj.items()}

                memory = create_memory_chunk(
                    user_id=user_id,
                    subject=subject_display,
                    predicate=predicate,
                    obj=obj,
                    memory_type=MemoryType.PERSONAL_FACT,
                    chat_id=context.get("chat_id"),
                    importance=ImportanceLevel.HIGH,
                    confidence=ConfidenceLevel.HIGH
                )

                memories.append(memory)

        return memories

    def _extract_preferences(
        self,
        text: str,
        user_id: str,
        timestamp: float,
        context: Dict[str, Any],
        subject_display: str
    ) -> List[MemoryChunk]:
        """æå–åå¥½ä¿¡æ¯"""
        memories = []

        # åå¥½æ¨¡å¼
        preference_patterns = [
            (r"æˆ‘å–œæ¬¢(.+)", "likes"),
            (r"æˆ‘ä¸å–œæ¬¢(.+)", "dislikes"),
            (r"æˆ‘çˆ±åƒ(.+)", "likes_food"),
            (r"æˆ‘è®¨åŒ(.+)", "hates"),
            (r"æˆ‘æœ€å–œæ¬¢çš„(.+)", "favorite_is"),
        ]

        for pattern, predicate in preference_patterns:
            match = re.search(pattern, text)
            if match:
                memory = create_memory_chunk(
                    user_id=user_id,
                    subject=subject_display,
                    predicate=predicate,
                    obj=match.group(1),
                    memory_type=MemoryType.PREFERENCE,
                    chat_id=context.get("chat_id"),
                    importance=ImportanceLevel.NORMAL,
                    confidence=ConfidenceLevel.MEDIUM
                )

                memories.append(memory)

        return memories

    def _extract_events(
        self,
        text: str,
        user_id: str,
        timestamp: float,
        context: Dict[str, Any],
        subject_display: str
    ) -> List[MemoryChunk]:
        """æå–äº‹ä»¶ä¿¡æ¯"""
        memories = []

        # äº‹ä»¶å…³é”®è¯
        event_keywords = ["æ˜å¤©", "ä»Šå¤©", "æ˜¨å¤©", "ä¸Šå‘¨", "ä¸‹å‘¨", "çº¦ä¼š", "ä¼šè®®", "æ´»åŠ¨", "æ—…è¡Œ", "ç”Ÿæ—¥"]

        if any(keyword in text for keyword in event_keywords):
            memory = create_memory_chunk(
                user_id=user_id,
                    subject=subject_display,
                predicate="mentioned_event",
                obj={"event_text": text, "timestamp": timestamp},
                memory_type=MemoryType.EVENT,
                chat_id=context.get("chat_id"),
                importance=ImportanceLevel.NORMAL,
                confidence=ConfidenceLevel.MEDIUM
            )

            memories.append(memory)

        return memories

    def _merge_hybrid_results(
        self,
        rule_memories: List[MemoryChunk],
        llm_memories: List[MemoryChunk]
    ) -> List[MemoryChunk]:
        """åˆå¹¶æ··åˆç­–ç•¥ç»“æœ"""
        all_memories = rule_memories.copy()

        # æ·»åŠ LLMè®°å¿†ï¼Œé¿å…é‡å¤
        for llm_memory in llm_memories:
            is_duplicate = False
            for rule_memory in rule_memories:
                if llm_memory.is_similar_to(rule_memory, threshold=0.7):
                    is_duplicate = True
                    # åˆå¹¶ç½®ä¿¡åº¦
                    rule_memory.metadata.confidence = ConfidenceLevel(
                        max(rule_memory.metadata.confidence.value, llm_memory.metadata.confidence.value)
                    )
                    break

            if not is_duplicate:
                all_memories.append(llm_memory)

        return all_memories

    def _validate_and_enhance_memories(
        self,
        memories: List[MemoryChunk],
        context: Dict[str, Any]
    ) -> List[MemoryChunk]:
        """éªŒè¯å’Œå¢å¼ºè®°å¿†"""
        validated_memories = []

        for memory in memories:
            # åŸºæœ¬éªŒè¯
            if not self._validate_memory(memory):
                continue

            # å¢å¼ºè®°å¿†
            enhanced_memory = self._enhance_memory(memory, context)
            validated_memories.append(enhanced_memory)

        return validated_memories

    def _validate_memory(self, memory: MemoryChunk) -> bool:
        """éªŒè¯è®°å¿†å—"""
        # æ£€æŸ¥åŸºæœ¬å­—æ®µ
        if not memory.content.subject or not memory.content.predicate:
            logger.debug(f"è®°å¿†å—ç¼ºå°‘ä¸»è¯­æˆ–è°“è¯­: {memory.memory_id}")
            return False

        # æ£€æŸ¥å†…å®¹é•¿åº¦
        content_length = len(memory.text_content)
        if content_length < 5 or content_length > 500:
            logger.debug(f"è®°å¿†å—å†…å®¹é•¿åº¦å¼‚å¸¸: {content_length}")
            return False

        # æ£€æŸ¥ç½®ä¿¡åº¦
        if memory.metadata.confidence == ConfidenceLevel.LOW:
            logger.debug(f"è®°å¿†å—ç½®ä¿¡åº¦è¿‡ä½: {memory.memory_id}")
            return False

        return True

    def _enhance_memory(
        self,
        memory: MemoryChunk,
        context: Dict[str, Any]
    ) -> MemoryChunk:
        """å¢å¼ºè®°å¿†å—"""
        # æ—¶é—´è§„èŒƒåŒ–å¤„ç†
        self._normalize_time_in_memory(memory)

        # æ·»åŠ æ—¶é—´ä¸Šä¸‹æ–‡
        if not memory.temporal_context:
            memory.temporal_context = {
                "timestamp": memory.metadata.created_at,
                "timezone": context.get("timezone", "UTC"),
                "day_of_week": datetime.fromtimestamp(memory.metadata.created_at).strftime("%A")
            }

        # æ·»åŠ æƒ…æ„Ÿä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if context.get("sentiment"):
            memory.metadata.emotional_context = context["sentiment"]

        # è‡ªåŠ¨æ·»åŠ æ ‡ç­¾
        self._auto_tag_memory(memory)

        return memory

    def _normalize_time_in_memory(self, memory: MemoryChunk):
        """è§„èŒƒåŒ–è®°å¿†ä¸­çš„æ—¶é—´è¡¨è¾¾"""
        import re
        from datetime import datetime, timedelta

        # è·å–å½“å‰æ—¶é—´ä½œä¸ºå‚è€ƒ
        current_time = datetime.fromtimestamp(memory.metadata.created_at)

        # å®šä¹‰ç›¸å¯¹æ—¶é—´æ˜ å°„
        relative_time_patterns = {
            r'ä»Šå¤©|ä»Šæ—¥': current_time.strftime('%Y-%m-%d'),
            r'æ˜¨å¤©|æ˜¨æ—¥': (current_time - timedelta(days=1)).strftime('%Y-%m-%d'),
            r'æ˜å¤©|æ˜æ—¥': (current_time + timedelta(days=1)).strftime('%Y-%m-%d'),
            r'åå¤©': (current_time + timedelta(days=2)).strftime('%Y-%m-%d'),
            r'å¤§åå¤©': (current_time + timedelta(days=3)).strftime('%Y-%m-%d'),
            r'å‰å¤©': (current_time - timedelta(days=2)).strftime('%Y-%m-%d'),
            r'å¤§å‰å¤©': (current_time - timedelta(days=3)).strftime('%Y-%m-%d'),
            r'æœ¬å‘¨|è¿™å‘¨|è¿™æ˜ŸæœŸ': current_time.strftime('%Y-%m-%d'),
            r'ä¸Šå‘¨|ä¸Šæ˜ŸæœŸ': (current_time - timedelta(weeks=1)).strftime('%Y-%m-%d'),
            r'ä¸‹å‘¨|ä¸‹æ˜ŸæœŸ': (current_time + timedelta(weeks=1)).strftime('%Y-%m-%d'),
            r'æœ¬æœˆ|è¿™ä¸ªæœˆ': current_time.strftime('%Y-%m-01'),
            r'ä¸Šæœˆ|ä¸Šä¸ªæœˆ': (current_time.replace(day=1) - timedelta(days=1)).strftime('%Y-%m-01'),
            r'ä¸‹æœˆ|ä¸‹ä¸ªæœˆ': (current_time.replace(day=1) + timedelta(days=32)).replace(day=1).strftime('%Y-%m-01'),
            r'ä»Šå¹´|ä»Šå¹´': current_time.strftime('%Y'),
            r'å»å¹´|ä¸Šä¸€å¹´': str(current_time.year - 1),
            r'æ˜å¹´|ä¸‹ä¸€å¹´': str(current_time.year + 1),
        }

        def _normalize_value(value):
            if isinstance(value, str):
                normalized = value
                for pattern, replacement in relative_time_patterns.items():
                    normalized = re.sub(pattern, replacement, normalized)
                return normalized
            if isinstance(value, dict):
                return {k: _normalize_value(v) for k, v in value.items()}
            if isinstance(value, list):
                return [_normalize_value(item) for item in value]
            return value

        # è§„èŒƒåŒ–ä¸»è¯­å’Œè°“è¯­ï¼ˆé€šå¸¸æ˜¯å­—ç¬¦ä¸²ï¼‰
        memory.content.subject = _normalize_value(memory.content.subject)
        memory.content.predicate = _normalize_value(memory.content.predicate)

        # è§„èŒƒåŒ–å®¾è¯­ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–å­—å…¸ï¼‰
        memory.content.object = _normalize_value(memory.content.object)

        # è®°å½•æ—¶é—´è§„èŒƒåŒ–æ“ä½œ
        logger.debug(f"è®°å¿† {memory.memory_id} å·²è¿›è¡Œæ—¶é—´è§„èŒƒåŒ–")

    def _auto_tag_memory(self, memory: MemoryChunk):
        """è‡ªåŠ¨ä¸ºè®°å¿†æ·»åŠ æ ‡ç­¾"""
        # åŸºäºè®°å¿†ç±»å‹çš„è‡ªåŠ¨æ ‡ç­¾
        type_tags = {
            MemoryType.PERSONAL_FACT: ["ä¸ªäººä¿¡æ¯", "åŸºæœ¬èµ„æ–™"],
            MemoryType.EVENT: ["äº‹ä»¶", "æ—¥ç¨‹"],
            MemoryType.PREFERENCE: ["åå¥½", "å–œå¥½"],
            MemoryType.OPINION: ["è§‚ç‚¹", "æ€åº¦"],
            MemoryType.RELATIONSHIP: ["å…³ç³»", "ç¤¾äº¤"],
            MemoryType.EMOTION: ["æƒ…æ„Ÿ", "æƒ…ç»ª"],
            MemoryType.KNOWLEDGE: ["çŸ¥è¯†", "ä¿¡æ¯"],
            MemoryType.SKILL: ["æŠ€èƒ½", "èƒ½åŠ›"],
            MemoryType.GOAL: ["ç›®æ ‡", "è®¡åˆ’"],
            MemoryType.EXPERIENCE: ["ç»éªŒ", "ç»å†"],
        }

        tags = type_tags.get(memory.memory_type, [])
        for tag in tags:
            memory.add_tag(tag)

    def _update_extraction_stats(self, success_count: int, extraction_time: float):
        """æ›´æ–°æå–ç»Ÿè®¡"""
        self.extraction_stats["total_extractions"] += 1
        self.extraction_stats["successful_extractions"] += success_count
        self.extraction_stats["failed_extractions"] += max(0, 1 - success_count)

        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        if self.extraction_stats["successful_extractions"] > 0:
            total_confidence = self.extraction_stats["average_confidence"] * (self.extraction_stats["successful_extractions"] - success_count)
            # å‡è®¾æ–°è®°å¿†çš„å¹³å‡ç½®ä¿¡åº¦ä¸º0.8
            total_confidence += 0.8 * success_count
            self.extraction_stats["average_confidence"] = total_confidence / self.extraction_stats["successful_extractions"]

    def get_extraction_stats(self) -> Dict[str, Any]:
        """è·å–æå–ç»Ÿè®¡ä¿¡æ¯"""
        return self.extraction_stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.extraction_stats = {
            "total_extractions": 0,
            "successful_extractions": 0,
            "failed_extractions": 0,
            "average_confidence": 0.0
        }