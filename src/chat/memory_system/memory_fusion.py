# -*- coding: utf-8 -*-
"""
记忆融合与去重机制
避免记忆碎片化，确保长期记忆库的高质量
"""

import time
import hashlib
from typing import Dict, List, Optional, Tuple, Set, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import defaultdict
import asyncio

from src.common.logger import get_logger
from src.chat.memory_system.memory_chunk import (
    MemoryChunk, MemoryType, ConfidenceLevel, ImportanceLevel
)

logger = get_logger(__name__)


@dataclass
class FusionResult:
    """融合结果"""
    original_count: int
    fused_count: int
    removed_duplicates: int
    merged_memories: List[MemoryChunk]
    fusion_time: float
    details: List[str]


@dataclass
class DuplicateGroup:
    """重复记忆组"""
    group_id: str
    memories: List[MemoryChunk]
    similarity_matrix: List[List[float]]
    representative_memory: Optional[MemoryChunk] = None


class MemoryFusionEngine:
    """记忆融合引擎"""

    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.fusion_stats = {
            "total_fusions": 0,
            "memories_fused": 0,
            "duplicates_removed": 0,
            "average_similarity": 0.0
        }

        # 融合策略配置
        self.fusion_strategies = {
            "semantic_similarity": True,      # 语义相似性融合
            "temporal_proximity": True,         # 时间接近性融合
            "logical_consistency": True,       # 逻辑一致性融合
            "confidence_boosting": True,        # 置信度提升
            "importance_preservation": True     # 重要性保持
        }

    async def fuse_memories(
        self,
        new_memories: List[MemoryChunk],
        existing_memories: Optional[List[MemoryChunk]] = None
    ) -> List[MemoryChunk]:
        """融合记忆列表"""
        start_time = time.time()

        try:
            if not new_memories:
                return []

            logger.info(f"开始记忆融合，新记忆: {len(new_memories)}，现有记忆: {len(existing_memories or [])}")

            # 1. 检测重复记忆组
            duplicate_groups = await self._detect_duplicate_groups(
                new_memories, existing_memories or []
            )

            # 2. 对每个重复组进行融合
            fused_memories = []
            removed_count = 0

            for group in duplicate_groups:
                if len(group.memories) == 1:
                    # 单个记忆，直接添加
                    fused_memories.append(group.memories[0])
                else:
                    # 多个记忆，进行融合
                    fused_memory = await self._fuse_memory_group(group)
                    if fused_memory:
                        fused_memories.append(fused_memory)
                        removed_count += len(group.memories) - 1

            # 3. 更新统计
            fusion_time = time.time() - start_time
            self._update_fusion_stats(len(new_memories), removed_count, fusion_time)

            logger.info(f"✅ 记忆融合完成: {len(fused_memories)} 条记忆，移除 {removed_count} 条重复")
            return fused_memories

        except Exception as e:
            logger.error(f"❌ 记忆融合失败: {e}", exc_info=True)
            return new_memories  # 失败时返回原始记忆

    async def _detect_duplicate_groups(
        self,
        new_memories: List[MemoryChunk],
        existing_memories: List[MemoryChunk]
    ) -> List[DuplicateGroup]:
        """检测重复记忆组"""
        all_memories = new_memories + existing_memories
        groups = []
        processed_ids = set()

        for i, memory1 in enumerate(all_memories):
            if memory1.memory_id in processed_ids:
                continue

            # 创建新的重复组
            group = DuplicateGroup(
                group_id=f"group_{len(groups)}",
                memories=[memory1],
                similarity_matrix=[[1.0]]
            )

            processed_ids.add(memory1.memory_id)

            # 寻找相似记忆
            for j, memory2 in enumerate(all_memories[i+1:], i+1):
                if memory2.memory_id in processed_ids:
                    continue

                similarity = self._calculate_comprehensive_similarity(memory1, memory2)

                if similarity >= self.similarity_threshold:
                    group.memories.append(memory2)
                    processed_ids.add(memory2.memory_id)

                    # 更新相似度矩阵
                    self._update_similarity_matrix(group, memory2, similarity)

            if len(group.memories) > 1:
                # 选择代表性记忆
                group.representative_memory = self._select_representative_memory(group)
                groups.append(group)

        logger.debug(f"检测到 {len(groups)} 个重复记忆组")
        return groups

    def _calculate_comprehensive_similarity(self, mem1: MemoryChunk, mem2: MemoryChunk) -> float:
        """计算综合相似度"""
        similarity_scores = []

        # 1. 语义向量相似度
        if self.fusion_strategies["semantic_similarity"]:
            semantic_sim = mem1.calculate_similarity(mem2)
            similarity_scores.append(("semantic", semantic_sim))

        # 2. 文本相似度
        text_sim = self._calculate_text_similarity(mem1.text_content, mem2.text_content)
        similarity_scores.append(("text", text_sim))

        # 3. 关键词重叠度
        keyword_sim = self._calculate_keyword_similarity(mem1.keywords, mem2.keywords)
        similarity_scores.append(("keyword", keyword_sim))

        # 4. 类型一致性
        type_consistency = 1.0 if mem1.memory_type == mem2.memory_type else 0.0
        similarity_scores.append(("type", type_consistency))

        # 5. 时间接近性
        if self.fusion_strategies["temporal_proximity"]:
            temporal_sim = self._calculate_temporal_similarity(
                mem1.metadata.created_at, mem2.metadata.created_at
            )
            similarity_scores.append(("temporal", temporal_sim))

        # 6. 逻辑一致性
        if self.fusion_strategies["logical_consistency"]:
            logical_sim = self._calculate_logical_similarity(mem1, mem2)
            similarity_scores.append(("logical", logical_sim))

        # 计算加权平均相似度
        weights = {
            "semantic": 0.35,
            "text": 0.25,
            "keyword": 0.15,
            "type": 0.10,
            "temporal": 0.10,
            "logical": 0.05
        }

        weighted_sum = 0.0
        total_weight = 0.0

        for score_type, score in similarity_scores:
            weight = weights.get(score_type, 0.1)
            weighted_sum += weight * score
            total_weight += weight

        final_similarity = weighted_sum / total_weight if total_weight > 0 else 0.0

        logger.debug(f"综合相似度计算: {final_similarity:.3f} - {[(t, f'{s:.3f}') for t, s in similarity_scores]}")

        return final_similarity

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """计算文本相似度"""
        # 简单的词汇重叠度计算
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        jaccard_similarity = len(intersection) / len(union)
        return jaccard_similarity

    def _calculate_keyword_similarity(self, keywords1: List[str], keywords2: List[str]) -> float:
        """计算关键词相似度"""
        if not keywords1 or not keywords2:
            return 0.0

        set1 = set(k.lower() for k in keywords1)
        set2 = set(k.lower() for k in keywords2)

        intersection = set1 & set2
        union = set1 | set2

        return len(intersection) / len(union) if union else 0.0

    def _calculate_temporal_similarity(self, time1: float, time2: float) -> float:
        """计算时间相似度"""
        time_diff = abs(time1 - time2)
        hours_diff = time_diff / 3600

        # 24小时内相似度较高
        if hours_diff <= 24:
            return 1.0 - (hours_diff / 24)
        elif hours_diff <= 168:  # 一周内
            return 0.7 - ((hours_diff - 24) / 168) * 0.5
        else:
            return 0.2

    def _calculate_logical_similarity(self, mem1: MemoryChunk, mem2: MemoryChunk) -> float:
        """计算逻辑一致性"""
        # 检查主谓宾结构的逻辑一致性
        consistency_score = 0.0

        # 主语一致性
        if mem1.content.subject == mem2.content.subject:
            consistency_score += 0.4

        # 谓语相似性
        predicate_sim = self._calculate_text_similarity(mem1.content.predicate, mem2.content.predicate)
        consistency_score += predicate_sim * 0.3

        # 宾语相似性
        if isinstance(mem1.content.object, str) and isinstance(mem2.content.object, str):
            object_sim = self._calculate_text_similarity(
                str(mem1.content.object), str(mem2.content.object)
            )
            consistency_score += object_sim * 0.3

        return consistency_score

    def _update_similarity_matrix(self, group: DuplicateGroup, new_memory: MemoryChunk, similarity: float):
        """更新组的相似度矩阵"""
        # 为新记忆添加行和列
        for i in range(len(group.similarity_matrix)):
            group.similarity_matrix[i].append(similarity)

        # 添加新行
        new_row = [similarity] + [1.0] * len(group.similarity_matrix)
        group.similarity_matrix.append(new_row)

    def _select_representative_memory(self, group: DuplicateGroup) -> MemoryChunk:
        """选择代表性记忆"""
        if not group.memories:
            return None

        # 评分标准
        best_memory = None
        best_score = -1.0

        for memory in group.memories:
            score = 0.0

            # 置信度权重
            score += memory.metadata.confidence.value * 0.3

            # 重要性权重
            score += memory.metadata.importance.value * 0.3

            # 访问次数权重
            score += min(memory.metadata.access_count * 0.1, 0.2)

            # 相关度权重
            score += memory.metadata.relevance_score * 0.2

            if score > best_score:
                best_score = score
                best_memory = memory

        return best_memory

    async def _fuse_memory_group(self, group: DuplicateGroup) -> Optional[MemoryChunk]:
        """融合记忆组"""
        if not group.memories:
            return None

        if len(group.memories) == 1:
            return group.memories[0]

        try:
            # 选择基础记忆（通常是代表性记忆）
            base_memory = group.representative_memory or group.memories[0]

            # 融合其他记忆的属性
            fused_memory = await self._merge_memory_attributes(base_memory, group.memories)

            # 更新元数据
            self._update_fused_metadata(fused_memory, group)

            logger.debug(f"成功融合记忆组，包含 {len(group.memories)} 条原始记忆")
            return fused_memory

        except Exception as e:
            logger.error(f"融合记忆组失败: {e}")
            # 返回置信度最高的记忆
            return max(group.memories, key=lambda m: m.metadata.confidence.value)

    async def _merge_memory_attributes(
        self,
        base_memory: MemoryChunk,
        memories: List[MemoryChunk]
    ) -> MemoryChunk:
        """合并记忆属性"""
        # 创建基础记忆的深拷贝
        fused_memory = MemoryChunk.from_dict(base_memory.to_dict())

        # 合并关键词
        all_keywords = set()
        for memory in memories:
            all_keywords.update(memory.keywords)
        fused_memory.keywords = sorted(all_keywords)

        # 合并标签
        all_tags = set()
        for memory in memories:
            all_tags.update(memory.tags)
        fused_memory.tags = sorted(all_tags)

        # 合并分类
        all_categories = set()
        for memory in memories:
            all_categories.update(memory.categories)
        fused_memory.categories = sorted(all_categories)

        # 合并关联记忆
        all_related = set()
        for memory in memories:
            all_related.update(memory.related_memories)
        # 移除对自身和组内记忆的引用
        all_related = {rid for rid in all_related if rid not in [m.memory_id for m in memories]}
        fused_memory.related_memories = sorted(all_related)

        # 合并时间上下文
        if self.fusion_strategies["temporal_proximity"]:
            fused_memory.temporal_context = self._merge_temporal_context(memories)

        return fused_memory

    def _update_fused_metadata(self, fused_memory: MemoryChunk, group: DuplicateGroup):
        """更新融合记忆的元数据"""
        # 更新修改时间
        fused_memory.metadata.last_modified = time.time()

        # 计算平均访问次数
        total_access = sum(m.metadata.access_count for m in group.memories)
        fused_memory.metadata.access_count = total_access

        # 提升置信度（如果有多个来源支持）
        if self.fusion_strategies["confidence_boosting"] and len(group.memories) > 1:
            max_confidence = max(m.metadata.confidence.value for m in group.memories)
            if max_confidence < ConfidenceLevel.VERIFIED.value:
                fused_memory.metadata.confidence = ConfidenceLevel(
                    min(max_confidence + 1, ConfidenceLevel.VERIFIED.value)
                )

        # 保持最高重要性
        if self.fusion_strategies["importance_preservation"]:
            max_importance = max(m.metadata.importance.value for m in group.memories)
            fused_memory.metadata.importance = ImportanceLevel(max_importance)

        # 计算平均相关度
        avg_relevance = sum(m.metadata.relevance_score for m in group.memories) / len(group.memories)
        fused_memory.metadata.relevance_score = min(avg_relevance * 1.1, 1.0)  # 稍微提升相关度

        # 设置来源信息
        source_ids = [m.memory_id[:8] for m in group.memories]
        fused_memory.metadata.source_context = f"Fused from {len(group.memories)} memories: {', '.join(source_ids)}"

    def _merge_temporal_context(self, memories: List[MemoryChunk]) -> Dict[str, Any]:
        """合并时间上下文"""
        contexts = [m.temporal_context for m in memories if m.temporal_context]

        if not contexts:
            return {}

        # 计算时间范围
        timestamps = [m.metadata.created_at for m in memories]
        earliest_time = min(timestamps)
        latest_time = max(timestamps)

        merged_context = {
            "earliest_timestamp": earliest_time,
            "latest_timestamp": latest_time,
            "time_span_hours": (latest_time - earliest_time) / 3600,
            "source_memories": len(memories)
        }

        # 合并其他上下文信息
        for context in contexts:
            for key, value in context.items():
                if key not in ["timestamp", "earliest_timestamp", "latest_timestamp"]:
                    if key not in merged_context:
                        merged_context[key] = value
                    elif merged_context[key] != value:
                        merged_context[key] = f"multiple: {value}"

        return merged_context

    async def incremental_fusion(
        self,
        new_memory: MemoryChunk,
        existing_memories: List[MemoryChunk]
    ) -> Tuple[MemoryChunk, List[MemoryChunk]]:
        """增量融合（单个新记忆与现有记忆融合）"""
        # 寻找相似记忆
        similar_memories = []

        for existing in existing_memories:
            similarity = self._calculate_comprehensive_similarity(new_memory, existing)
            if similarity >= self.similarity_threshold:
                similar_memories.append((existing, similarity))

        if not similar_memories:
            # 没有相似记忆，直接返回
            return new_memory, existing_memories

        # 按相似度排序
        similar_memories.sort(key=lambda x: x[1], reverse=True)

        # 与最相似的记忆融合
        best_match, similarity = similar_memories[0]

        # 创建融合组
        group = DuplicateGroup(
            group_id=f"incremental_{int(time.time())}",
            memories=[new_memory, best_match],
            similarity_matrix=[[1.0, similarity], [similarity, 1.0]]
        )

        # 执行融合
        fused_memory = await self._fuse_memory_group(group)

        # 从现有记忆中移除被融合的记忆
        updated_existing = [m for m in existing_memories if m.memory_id != best_match.memory_id]
        updated_existing.append(fused_memory)

        logger.debug(f"增量融合完成，相似度: {similarity:.3f}")

        return fused_memory, updated_existing

    def _update_fusion_stats(self, original_count: int, removed_count: int, fusion_time: float):
        """更新融合统计"""
        self.fusion_stats["total_fusions"] += 1
        self.fusion_stats["memories_fused"] += original_count
        self.fusion_stats["duplicates_removed"] += removed_count

        # 更新平均相似度（估算）
        if removed_count > 0:
            avg_similarity = 0.9  # 假设平均相似度较高
            total_similarity = self.fusion_stats["average_similarity"] * (self.fusion_stats["total_fusions"] - 1)
            total_similarity += avg_similarity
            self.fusion_stats["average_similarity"] = total_similarity / self.fusion_stats["total_fusions"]

    async def maintenance(self):
        """维护操作"""
        try:
            logger.info("开始记忆融合引擎维护...")

            # 可以在这里添加定期维护任务，如：
            # - 重新评估低置信度记忆
            # - 清理孤立记忆引用
            # - 优化融合策略参数

            logger.info("✅ 记忆融合引擎维护完成")

        except Exception as e:
            logger.error(f"❌ 记忆融合引擎维护失败: {e}", exc_info=True)

    def get_fusion_stats(self) -> Dict[str, Any]:
        """获取融合统计信息"""
        return self.fusion_stats.copy()

    def reset_stats(self):
        """重置统计信息"""
        self.fusion_stats = {
            "total_fusions": 0,
            "memories_fused": 0,
            "duplicates_removed": 0,
            "average_similarity": 0.0
        }